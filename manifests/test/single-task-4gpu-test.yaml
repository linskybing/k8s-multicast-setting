apiVersion: v1
kind: Pod
metadata:
  name: multi-gpu-task
  labels:
    test: multi-gpu
spec:
  restartPolicy: Never
  nodeSelector:
    kubernetes.io/hostname: gpu1 # 指定那台有 4 張卡的機器
  containers:
  - name: cuda-app
    image: nvidia/cuda:12.4.1-devel-ubuntu22.04
    env:
      # 如果你想測試 MPS 限制，請保留這兩行；若想跑全速，請註解掉或設為 100
      - name: CUDA_MPS_ACTIVE_THREAD_PERCENTAGE
        value: "20" 
      - name: CUDA_MPS_PINNED_DEVICE_MEM_LIMIT
        value: "0:6400M"
    resources:
      limits:
        nvidia.com/gpu: 4  # <--- 重點：我要 4 張卡！
    command: ["/bin/bash", "-c"]
    args:
      - |
        cat <<EOF > multigpu_test.cu
        #include <cuda_runtime.h>
        #include <iostream>
        #include <vector>
        #include <cmath>
        #include <omp.h> // 使用 OpenMP 做並行控制 (若編譯器支援)

        #define N 4096

        __global__ void heavy_kernel(float *a, int n) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx < n * n) {
                float val = 0.0f;
                for (int k = 0; k < 1000; ++k) {
                    val += sinf(idx * k) * cosf(idx * k);
                }
                a[idx] = val;
            }
        }

        int main() {
            int deviceCount = 0;
            cudaGetDeviceCount(&deviceCount);

            std::cout << "=== Detected " << deviceCount << " GPUs available to this Task ===" << std::endl;

            if (deviceCount < 4) {
                std::cerr << "Warning: Requested 4 GPUs but only found " << deviceCount << std::endl;
            }

            // 依序在每一張卡上啟動任務
            for (int i = 0; i < deviceCount; ++i) {
                cudaSetDevice(i); // 切換當前控制的 GPU
                
                cudaDeviceProp prop;
                cudaGetDeviceProperties(&prop, i);
                std::cout << "[GPU " << i << "] " << prop.name 
                          << " (BusID: " << prop.pciBusID << ") - Launching Work..." << std::endl;

                float *d_a;
                size_t size = N * N * sizeof(float);
                cudaMalloc(&d_a, size);

                // 啟動 Kernel
                heavy_kernel<<<(N*N+255)/256, 256>>>(d_a, N);
                
                // 不等待，直接換下一張卡發命令 (非同步啟動)
            }

            std::cout << "All kernels launched on 4 GPUs. Waiting for completion..." << std::endl;

            //再一次迴圈等待所有卡完成
            for (int i = 0; i < deviceCount; ++i) {
                cudaSetDevice(i);
                cudaDeviceSynchronize();
                std::cout << "[GPU " << i << "] Work Completed!" << std::endl;
            }

            return 0;
        }
        EOF

        echo "Compiling..."
        nvcc multigpu_test.cu -o multigpu_test -O3
        echo "Running Multi-GPU Task..."
        ./multigpu_test